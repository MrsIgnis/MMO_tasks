{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPXgFqH234FJgm5D7yvbiYX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MrsIgnis/MMO_tasks/blob/main/MOCI_task_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# IV. Решение задач. Классификация. Кластеризация. Генерация"
      ],
      "metadata": {
        "id": "iJ7_JZ1regqD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Задание: используя полученные знания, пройти этапы с *collect data* до *split data*. Т. е. подобрать датасет (в котором будут текста и их категории), загрузить, провести предоработку, проанализировать, векторизовать, провести кластеризацию, сравнить результаты с реальной разметкой и в результате разбить на train, test и val выборки.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "3aNrvUj9es7R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "> Для кластеризации я выбрала BERTopic, поэтому этап предоработки данных более мягкий в сравнении с предыдущими л/р, а отдельный этап векторизации не нужен (BERTopic сам может это сделать).\n",
        "\n"
      ],
      "metadata": {
        "id": "MTslFn9uA_-c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**0. Загрузка библиотек**"
      ],
      "metadata": {
        "id": "L6r0OemfgCnN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7S9_ngR_ddzW"
      },
      "outputs": [],
      "source": [
        "!pip install nltk pymorphy3\n",
        "!pip install bertopic sentence-transformers hdbscan umap-learn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import math\n",
        "import re\n",
        "import nltk\n",
        "import pymorphy3\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from bertopic import BERTopic\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from umap import UMAP\n",
        "from hdbscan import HDBSCAN\n",
        "from collections import Counter\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score\n",
        "from sklearn.metrics import homogeneity_score, completeness_score, v_measure_score"
      ],
      "metadata": {
        "id": "5D5MtlyvePQc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "xPVhRTgKf5SU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words_en = set(stopwords.words(\"english\"))"
      ],
      "metadata": {
        "id": "YS5ulrsef8eb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lemma_en = WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "TTvDqJo8f_hh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/pokemon-cards.csv')\n",
        "df"
      ],
      "metadata": {
        "id": "xMPhGPl5j_1n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.rename(columns={'caption': 'text', 'set_name': 'category'})\n",
        "df"
      ],
      "metadata": {
        "id": "OyQG2CxjoiPj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**I. Предоработка датасета**"
      ],
      "metadata": {
        "id": "noGTa6-Rowan"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def is_english(word: str) -> bool:\n",
        "    return bool(re.search('[a-z]', word, re.IGNORECASE))"
      ],
      "metadata": {
        "id": "YYG8-w5xo1dp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text: str) -> str:\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^а-яёa-z\\s]', '', text, flags=re.IGNORECASE)\n",
        "    words = word_tokenize(text)\n",
        "    lemmas = []\n",
        "    for word in words:\n",
        "        if word not in stop_words_en:\n",
        "            if is_english(word):\n",
        "                lemma = lemma_en.lemmatize(word)\n",
        "            else:\n",
        "                lemma = word\n",
        "            lemmas.append(lemma)\n",
        "    return ' '.join(lemmas)"
      ],
      "metadata": {
        "id": "fk9qnjSipW0l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['processed_text'] = df['text'].apply(preprocess_text)"
      ],
      "metadata": {
        "id": "kpzvfzy6qdFG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"--- Данные после предобработки: ---\")\n",
        "df[['text', 'processed_text', 'category']]"
      ],
      "metadata": {
        "id": "54wmt-2vqrOk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**II. Анализ датасета**"
      ],
      "metadata": {
        "id": "y8MQ2pMHrAFe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "category_counts = df['category'].value_counts()\n",
        "print(\"Распределение по категориям:\\n\")\n",
        "print(category_counts)"
      ],
      "metadata": {
        "id": "1Vn42_vhrC-R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(50, 10))\n",
        "sns.barplot(x=category_counts.index, y=category_counts.values)\n",
        "plt.title(\"Распределение текстов по категориям\")\n",
        "plt.xlabel(\"Категория\")\n",
        "plt.ylabel(\"Количество текстов\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "XZYzU6z5r8kk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['text_len_raw'] = df['text'].apply(len)\n",
        "df['text_len_processed'] = df['processed_text'].apply(len)"
      ],
      "metadata": {
        "id": "w7arz6hqtDUw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.histplot(df['text_len_raw'], kde=True)\n",
        "plt.title(\"Распределение длины исходных текстов\")\n",
        "plt.xlabel(\"Длина текста\")\n",
        "plt.ylabel(\"Частота\")"
      ],
      "metadata": {
        "id": "Wz7M8yR9tLR4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.subplot(1, 2, 2)\n",
        "sns.histplot(df['text_len_processed'], kde=True)\n",
        "plt.title(\"Распределение длины обработанных текстов\")\n",
        "plt.xlabel(\"Длина текста\")\n",
        "plt.ylabel(\"Частота\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ZakaUsYWtOuW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_words = \" \".join(df['processed_text']).split()\n",
        "word_counts = Counter(all_words)\n",
        "most_common_words = word_counts.most_common(15)\n",
        "print(\"Топ-15 наиболее частых слов (после предобработки):\")\n",
        "print(most_common_words)"
      ],
      "metadata": {
        "id": "7e6pZNi0ue_1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if most_common_words:\n",
        "    common_words_df = pd.DataFrame(most_common_words, columns=['word', 'count'])\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.barplot(x='count', y='word', data=common_words_df, palette='viridis')\n",
        "    plt.title(\"Топ-15 наиболее частых слов\")\n",
        "    plt.xlabel(\"Частота\")\n",
        "    plt.ylabel(\"Слово\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"Недостаточно слов для отображения частотности после предобработки (возможно, датасет слишком мал или все слова - стоп-слова).\")"
      ],
      "metadata": {
        "id": "baE763QNvRiw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**III. Кластеризация**"
      ],
      "metadata": {
        "id": "eA4HVcvnAL4l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "documents = df['processed_text'].tolist()\n",
        "true_categories = df['category'].tolist()"
      ],
      "metadata": {
        "id": "8tnn46o2AjFj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_model_name = 'all-MiniLM-L6-v2'"
      ],
      "metadata": {
        "id": "417vfbcyCK4W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence_model = SentenceTransformer(embedding_model_name)"
      ],
      "metadata": {
        "id": "tilctLbpCvV7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hdbscan_model = HDBSCAN(\n",
        "    min_cluster_size=3,\n",
        "    min_samples=1,\n",
        "    metric='euclidean',\n",
        "    cluster_selection_method='eom',\n",
        "    prediction_data=True\n",
        ")\n",
        "umap_model = UMAP(\n",
        "    n_neighbors=10,\n",
        "    n_components=3,\n",
        "    min_dist=0.0,\n",
        "    metric='cosine',\n",
        "    random_state=42\n",
        ")"
      ],
      "metadata": {
        "id": "ktMuVizRQudD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topic_model = BERTopic(\n",
        "        embedding_model=sentence_model,\n",
        "        language=\"english\",\n",
        "        min_topic_size=3,\n",
        "        nr_topics=150,\n",
        "        hdbscan_model=hdbscan_model,\n",
        "        umap_model=umap_model,\n",
        "        calculate_probabilities=True,\n",
        "        verbose=True\n",
        ")\n",
        "topics, probabilities = topic_model.fit_transform(documents)\n",
        "df['bertopic_cluster_label'] = topics\n",
        "print(\"Информация о найденных темах (BERTopic):\")\n",
        "topic_info_df = topic_model.get_topic_info()\n",
        "if not topic_info_df.empty:\n",
        "        print(topic_info_df)\n",
        "else:\n",
        "        print(\"BERTopic не нашел тем (кроме, возможно, выбросов).\")\n",
        "\n",
        "\n",
        "if -1 in set(topics) and len(set(topics)) == 1:\n",
        "        print(\"\\nBERTopic отнес все документы к выбросам (тема -1). Попробуйте изменить параметры, например, min_topic_size или настройки HDBSCAN/UMAP.\")\n",
        "else:\n",
        "        for topic_id in sorted(list(set(topics) - {-1})):\n",
        "            print(f\"\\nТема {topic_id}:\")\n",
        "            words = topic_model.get_topic(topic_id)\n",
        "            if words:\n",
        "                print([word for word, score in words[:15]])\n",
        "            else:\n",
        "                print(\"Нет репрезентативных слов для этой темы.\")"
      ],
      "metadata": {
        "id": "E3ClFTm4DCIu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**IV. Сравнение результатов кластеризации с реальной разметкой**"
      ],
      "metadata": {
        "id": "ppIMy8ElKtjD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if 'bertopic_cluster_label' in df.columns and df['bertopic_cluster_label'].nunique() > 0 and df['bertopic_cluster_label'].max() > -2:\n",
        "    clustered_mask = df['bertopic_cluster_label'] != -1\n",
        "    if clustered_mask.sum() > 0 and df.loc[clustered_mask, 'bertopic_cluster_label'].nunique() > 1 :\n",
        "        true_labels_filtered = df.loc[clustered_mask, 'category']\n",
        "        cluster_labels_filtered = df.loc[clustered_mask, 'bertopic_cluster_label']\n",
        "\n",
        "        ari_bertopic = adjusted_rand_score(true_labels_filtered, cluster_labels_filtered)\n",
        "        nmi_bertopic = normalized_mutual_info_score(true_labels_filtered, cluster_labels_filtered)\n",
        "        homogeneity_bt = homogeneity_score(true_labels_filtered, cluster_labels_filtered)\n",
        "        completeness_bt = completeness_score(true_labels_filtered, cluster_labels_filtered)\n",
        "        v_measure_bt = v_measure_score(true_labels_filtered, cluster_labels_filtered)\n",
        "\n",
        "        print(f\"Adjusted Rand Index (ARI) for BERTopic: {ari_bertopic:.3f}\")\n",
        "        print(f\"Normalized Mutual Information (NMI) for BERTopic: {nmi_bertopic:.3f}\")\n",
        "        print(f\"Homogeneity (BERTopic): {homogeneity_bt:.3f}\")\n",
        "        print(f\"Completeness (BERTopic): {completeness_bt:.3f}\")\n",
        "        print(f\"V-measure (BERTopic): {v_measure_bt:.3f}\")\n",
        "        print(f\"Количество документов, отнесенных к выбросам (тема -1): {(df['bertopic_cluster_label'] == -1).sum()}\")\n",
        "\n",
        "        contingency_table_bertopic = pd.crosstab(df['category'], df['bertopic_cluster_label'])\n",
        "        print(\"\\nТаблица сопряженности (реальные категории vs. BERTopic кластеры):\")\n",
        "        print(contingency_table_bertopic)\n",
        "    else:\n",
        "        print(\"Недостаточно кластеризованных данных (не считая выбросов) или кластеров для расчета метрик.\")\n",
        "        if 'bertopic_cluster_label' in df.columns:\n",
        "             print(f\"Уникальные метки кластеров BERTopic: {df['bertopic_cluster_label'].unique()}\")\n",
        "             print(f\"Количество документов с меткой -1 (outliers): {(df['bertopic_cluster_label'] == -1).sum()}\")\n",
        "else:\n",
        "    print(\"Кластеризация BERTopic не была выполнена или не дала результатов для сравнения.\")"
      ],
      "metadata": {
        "id": "7XemlzFtK2TK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if 'topic_model' in locals() and hasattr(topic_model, 'topics_') and -1 not in topic_model.get_topic_info()['Topic'].tolist() or len(topic_model.get_topic_info()) > 1:\n",
        "    print(\"\\nПопытка визуализации тем...\")\n",
        "    try:\n",
        "        fig_hierarchy = topic_model.visualize_hierarchy(top_n_topics=50)\n",
        "        if fig_hierarchy: fig_hierarchy.show()\n",
        "\n",
        "        fig_topics = topic_model.visualize_topics()\n",
        "        if fig_topics: fig_topics.show()\n",
        "\n",
        "        fig_barchart = topic_model.visualize_barchart(top_n_topics=min(10, len(topic_model.get_topic_info())-1 if -1 in topic_model.get_topic_info()['Topic'].tolist() else len(topic_model.get_topic_info())))\n",
        "        if fig_barchart: fig_barchart.show()\n",
        "\n",
        "        fig_heatmap = topic_model.visualize_heatmap(top_n_topics=20)\n",
        "        if fig_heatmap: fig_heatmap.show()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Ошибка при визуализации тем BERTopic: {e}\")\n",
        "else:\n",
        "    print(\"\\nМодель BERTopic не была успешно обучена или не нашла достаточно тем для основной визуализации.\")\n"
      ],
      "metadata": {
        "id": "_0r2CtLdOO0J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**V. Разбиение на train, test и val выборки**"
      ],
      "metadata": {
        "id": "w45wsiLINRqn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = df['text']\n",
        "y = df['category']"
      ],
      "metadata": {
        "id": "AlGIiemiNRH0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "min_samples_per_class = y.value_counts().min() if not y.empty else 0\n",
        "stratify_option_y = None\n",
        "stratify_option_y_train_val = None"
      ],
      "metadata": {
        "id": "vPcyslvmNk7g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if not y.empty and min_samples_per_class >= 2 :\n",
        "    stratify_option_y = y\n",
        "    X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
        "        X, y, test_size=0.2, random_state=42, stratify=stratify_option_y\n",
        "    )\n",
        "    if not y_train_val.empty and (y_train_val.value_counts().min() >= 2 or len(y_train_val.unique()) == 1):\n",
        "        stratify_option_y_train_val = y_train_val\n",
        "    else:\n",
        "        print(f\"\\nПредупреждение для второго сплита: В train_val классах менее 2 образцов или выборка пуста. Стратификация для val не будет применяться.\")\n",
        "        stratify_option_y_train_val = None\n",
        "\n",
        "    X_train, X_val, y_train, y_val = train_test_split(\n",
        "        X_train_val, y_train_val, test_size=0.25, random_state=42, stratify=stratify_option_y_train_val\n",
        "    )\n",
        "elif not y.empty:\n",
        "    print(f\"\\nПредупреждение: В некоторых классах менее 2 образцов (минимально: {min_samples_per_class}). Стратификация не применяется.\")\n",
        "    X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.25, random_state=42)\n",
        "else:\n",
        "    print(\"Целевая переменная y пуста. Разбиение на выборки не может быть выполнено.\")\n",
        "    X_train, X_val, X_test, y_train, y_val, y_test = (pd.Series(dtype='object'), pd.Series(dtype='object'), pd.Series(dtype='object'),\n",
        "                                                      pd.Series(dtype='object'), pd.Series(dtype='object'), pd.Series(dtype='object'))\n"
      ],
      "metadata": {
        "id": "GaNBkqq1NqyD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"--- Разбиение на выборки: ---\")\n",
        "print(f\"Размер исходного X: {X.shape}, y: {y.shape}\")\n",
        "print(f\"Размер X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
        "print(f\"Размер X_val: {X_val.shape}, y_val: {y_val.shape}\")\n",
        "print(f\"Размер X_test: {X_test.shape}, y_test: {y_test.shape}\")"
      ],
      "metadata": {
        "id": "dhvot3-0N1iR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if not y_train.empty: print(\"Распределение категорий в y_train:\\n\", y_train.value_counts(normalize=True))\n",
        "if not y_val.empty: print(\"\\nРаспределение категорий в y_val:\\n\", y_val.value_counts(normalize=True))\n",
        "if not y_test.empty: print(\"\\nРаспределение категорий в y_test:\\n\", y_test.value_counts(normalize=True))"
      ],
      "metadata": {
        "id": "Tj6I3DkOOAO9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}